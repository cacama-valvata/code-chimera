<p>Alright, I worked it out: an idea is to use the standard <code>multiprocessing</code> module and split the original array in just a few chunks (so as to limit communication overhead with the workers). This can be done relatively easily as follows:</p>
<pre><code>import multiprocessing

import numpy as np

def parallel_apply_along_axis(func1d, axis, arr, *args, **kwargs):
    &quot;&quot;&quot;
    Like numpy.apply_along_axis(), but takes advantage of multiple
    cores.
    &quot;&quot;&quot;        
    # Effective axis where apply_along_axis() will be applied by each
    # worker (any non-zero axis number would work, so as to allow the use
    # of `np.array_split()`, which is only done on axis 0):
    effective_axis = 1 if axis == 0 else axis
    if effective_axis != axis:
        arr = arr.swapaxes(axis, effective_axis)

    # Chunks for the mapping (only a few chunks):
    chunks = [(func1d, effective_axis, sub_arr, args, kwargs)
              for sub_arr in np.array_split(arr, multiprocessing.cpu_count())]

    pool = multiprocessing.Pool()
    individual_results = pool.map(unpacking_apply_along_axis, chunks)
    # Freeing the workers:
    pool.close()
    pool.join()

    return np.concatenate(individual_results)
</code></pre>
<p>where the function <code>unpacking_apply_along_axis()</code> being applied in <code>Pool.map()</code> is separate as it should (so that subprocesses can import it), and is simply a thin wrapper that handles the fact that <code>Pool.map()</code> only takes a single argument:</p>
<pre><code>def unpacking_apply_along_axis((func1d, axis, arr, args, kwargs)):
    &quot;&quot;&quot;
    Like numpy.apply_along_axis(), but with arguments in a tuple
    instead.

    This function is useful with multiprocessing.Pool().map(): (1)
    map() only handles functions that take a single argument, and (2)
    this function can generally be imported from a module, as required
    by map().
    &quot;&quot;&quot;
    return np.apply_along_axis(func1d, axis, arr, *args, **kwargs)
</code></pre>
<p>(in Python 3, the argument unpacking must be done manually:</p>
<pre><code>def unpacking_apply_along_axis(all_args):
    &quot;&quot;&quot;…&quot;&quot;&quot;
    (func1d, axis, arr, args, kwargs) = all_args
    …
</code></pre>
<p>because <a href="https://www.python.org/dev/peps/pep-3113/" rel="noreferrer">argument unpacking was removed</a>).</p>
<p>In my particular case, this resulted in a 2x speedup on 2 cores with hyper-threading. A factor closer to 4x would have been nicer, but the speed up is already nice, in just a few lines of code, and it should be better for machines with more cores (which are quite common). Maybe there is a way of avoiding data copies and using shared memory (maybe through the <a href="https://docs.python.org/2/library/multiprocessing.html#sharing-state-between-processes" rel="noreferrer"><code>multiprocessing</code> module</a> itself)?</p>
